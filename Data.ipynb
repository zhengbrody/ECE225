{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Data Processing\n",
    "\n",
    "Preprocessing pipeline for Amazon All_Beauty reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import subprocess\n",
    "import textwrap\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATEGORY = \"raw_review_All_Beauty\"\n",
    "MAX_REVIEWS = 200_000\n",
    "SEED = 42\n",
    "OUTPUT_DIR = Path(\"data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# External Python interpreter for dataset loading fallback\n",
    "EXTERNAL_PYTHON = \"/Library/Frameworks/Python.framework/Versions/3.12/bin/python3\"\n",
    "\n",
    "# Check dependencies\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    DATASETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DATASETS_AVAILABLE = False\n",
    "    print(\"Warning: datasets library not found\")\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    TEXTBLOB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TextBlob = None\n",
    "    TEXTBLOB_AVAILABLE = False\n",
    "    print(\"Warning: textblob not found, sentiment analysis disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cache_path(category, max_reviews):\n",
    "    limit = \"full\" if max_reviews is None else str(max_reviews)\n",
    "    safe_cat = category.replace(\"/\", \"_\")\n",
    "    return OUTPUT_DIR / f\"raw_{safe_cat}_{limit}.csv\"\n",
    "\n",
    "def _load_external(category, max_reviews):\n",
    "    \"\"\"Load dataset using external Python interpreter.\"\"\"\n",
    "    cache = _cache_path(category, max_reviews)\n",
    "    if cache.exists():\n",
    "        return pd.read_csv(cache)\n",
    "    \n",
    "    split = f\"full[:{max_reviews}]\" if max_reviews else \"full\"\n",
    "    script = textwrap.dedent(f'''\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"{category}\", split=\"{split}\", trust_remote_code=True)\n",
    "df = ds.to_pandas() if hasattr(ds, \"to_pandas\") else pd.DataFrame(ds)\n",
    "df.to_csv(\"{cache}\", index=False)\n",
    "''')\n",
    "    \n",
    "    subprocess.run([EXTERNAL_PYTHON, \"-c\", script], check=True, timeout=600)\n",
    "    return pd.read_csv(cache)\n",
    "\n",
    "def load_reviews(category, max_reviews, seed):\n",
    "    \"\"\"Load reviews from Amazon dataset with fallback strategies.\"\"\"\n",
    "    cache = _cache_path(category, max_reviews)\n",
    "    if cache.exists():\n",
    "        print(f\"Loading from cache: {cache}\")\n",
    "        return pd.read_csv(cache)\n",
    "    \n",
    "    split = f\"full[:{max_reviews}]\" if max_reviews else \"full\"\n",
    "    \n",
    "    if DATASETS_AVAILABLE:\n",
    "        try:\n",
    "            print(f\"Loading {category} [{split}]...\")\n",
    "            ds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", category, \n",
    "                            split=split, trust_remote_code=True)\n",
    "            df = ds.to_pandas() if hasattr(ds, \"to_pandas\") else pd.DataFrame(ds)\n",
    "            df.to_csv(cache, index=False)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Direct load failed: {e}\")\n",
    "            \n",
    "            if max_reviews is not None:\n",
    "                try:\n",
    "                    print(\"Trying streaming mode...\")\n",
    "                    stream = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", category,\n",
    "                                        split=\"full\", streaming=True, trust_remote_code=True)\n",
    "                    rows = list(islice(stream, max_reviews))\n",
    "                    df = pd.DataFrame(rows)\n",
    "                    df.to_csv(cache, index=False)\n",
    "                    return df\n",
    "                except Exception as e2:\n",
    "                    print(f\"Streaming failed: {e2}\")\n",
    "    \n",
    "    print(\"Using external interpreter...\")\n",
    "    return _load_external(category, max_reviews)\n",
    "\n",
    "def clean_reviews(df):\n",
    "    \"\"\"Remove invalid records and duplicates.\"\"\"\n",
    "    required = [\"rating\", \"user_id\", \"parent_asin\", \"timestamp\"]\n",
    "    df = df.dropna(subset=required)\n",
    "    \n",
    "    has_text = (\n",
    "        df[\"text\"].fillna(\"\").str.strip().ne(\"\") |\n",
    "        df[\"title\"].fillna(\"\").str.strip().ne(\"\")\n",
    "    )\n",
    "    df = df[has_text]\n",
    "    df = df[(df[\"rating\"] >= 1) & (df[\"rating\"] <= 5)]\n",
    "    \n",
    "    df[\"verified_purchase\"] = df.get(\"verified_purchase\", False).fillna(False).astype(bool)\n",
    "    df[\"helpful_vote\"] = df.get(\"helpful_vote\", 0).fillna(0)\n",
    "    df = df.drop_duplicates(subset=[\"user_id\", \"parent_asin\", \"text\", \"timestamp\"], keep=\"first\")\n",
    "    \n",
    "    df[\"review_time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"review_date\"] = df[\"review_time\"].dt.date\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    tokens = [t.lower() for t in text.split() if t.strip()]\n",
    "    return len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if not TEXTBLOB_AVAILABLE or not text.strip():\n",
    "        return 0.0\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def add_textual_features(df):\n",
    "    \"\"\"Engineer text-based indicators.\"\"\"\n",
    "    text = df[\"text\"].fillna(\"\")\n",
    "    title = df[\"title\"].fillna(\"\")\n",
    "    \n",
    "    df = df.assign(\n",
    "        text_length_chars=text.str.len(),\n",
    "        text_length_words=text.str.split().str.len(),\n",
    "        title_length_chars=title.str.len(),\n",
    "        uppercase_ratio=text.apply(lambda x: sum(c.isupper() for c in x) / max(len(x), 1)),\n",
    "        punctuation_ratio=text.apply(lambda x: sum(c in \"!?.,;:\" for c in x) / max(len(x), 1)),\n",
    "        sentiment=text.apply(get_sentiment),\n",
    "        lexical_diversity=text.apply(lexical_diversity)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_behavioral_features(df):\n",
    "    \"\"\"Aggregate user-level metrics.\"\"\"\n",
    "    df = df.sort_values([\"user_id\", \"review_time\"]).copy()\n",
    "    df[\"inter_review_seconds\"] = df.groupby(\"user_id\")[\"review_time\"].diff().dt.total_seconds()\n",
    "    \n",
    "    agg = df.groupby(\"user_id\").agg(\n",
    "        user_review_count=(\"rating\", \"size\"),\n",
    "        user_rating_mean=(\"rating\", \"mean\"),\n",
    "        user_rating_var=(\"rating\", \"var\"),\n",
    "        user_verified_ratio=(\"verified_purchase\", \"mean\"),\n",
    "        user_helpful_mean=(\"helpful_vote\", \"mean\"),\n",
    "        user_helpful_sum=(\"helpful_vote\", \"sum\"),\n",
    "        user_time_delta_median=(\"inter_review_seconds\", \"median\")\n",
    "    ).reset_index()\n",
    "    \n",
    "    agg = agg.fillna({\"user_rating_var\": 0.0, \"user_time_delta_median\": 0.0})\n",
    "    df = df.merge(agg, on=\"user_id\", how=\"left\")\n",
    "    df[\"rating_deviation_from_user_mean\"] = df[\"rating\"] - df[\"user_rating_mean\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = load_reviews(CATEGORY, MAX_REVIEWS, SEED)\n",
    "print(f\"Loaded {len(df_raw)} reviews\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_reviews(df_raw)\n",
    "print(f\"After cleaning: {len(df_clean)} reviews\")\n",
    "\n",
    "df_text = add_textual_features(df_clean)\n",
    "df_features = add_behavioral_features(df_text)\n",
    "\n",
    "print(f\"Added {len(df_features.columns) - len(df_raw.columns)} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"text_length_chars\", \"text_length_words\", \"sentiment\", \"lexical_diversity\",\n",
    "    \"user_review_count\", \"user_rating_var\", \"user_verified_ratio\",\n",
    "    \"user_helpful_sum\", \"user_time_delta_median\"\n",
    "]\n",
    "\n",
    "df_features[feature_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = OUTPUT_DIR / \"amazon_reviews_cleaned.csv\"\n",
    "user_path = OUTPUT_DIR / \"user_behavior_features.csv\"\n",
    "\n",
    "df_features.to_csv(review_path, index=False)\n",
    "\n",
    "user_cols = [\n",
    "    \"user_id\", \"user_review_count\", \"user_rating_mean\", \"user_rating_var\",\n",
    "    \"user_verified_ratio\", \"user_helpful_mean\", \"user_helpful_sum\",\n",
    "    \"user_time_delta_median\"\n",
    "]\n",
    "df_features[user_cols].drop_duplicates(\"user_id\").to_csv(user_path, index=False)\n",
    "\n",
    "print(f\"Saved review data: {review_path}\")\n",
    "print(f\"Saved user data: {user_path}\")\n",
    "print(f\"Unique users: {df_features['user_id'].nunique()}\")\n",
    "print(f\"Unique products: {df_features['parent_asin'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
